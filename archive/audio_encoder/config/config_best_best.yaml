# ───────────────────────── GLOBALS ──────────────────────────
seed:          1234

# ───────────────────────── DATA  ────────────────────────────
data:
  data_dir:             ${hydra:runtime.cwd}/src/data/
  last_shard_train:     "001339"                # last shard for training set 
  last_shard_val:       "000167"                # last shard for validation set
  last_shard_test:      "000002"                # last shard for test set     
  window:               1000                    # window size for WebDataset
  sr:                   16_000                  # sampling rate: 16_000 for Whisper
  num_workers:          8 
  prefetch_factor:      4 
  batch_size:           128 
  total_train_samples:  1_339_790
  total_val_samples:    167_484
  model_dir:            ${hydra:runtime.cwd}/src/models

# ───────────────────────── MODEL (LIE) ──────────────────────
model:
  _target_:         src.models.lie_best.LightLIE  
  device:           cuda:0
  debug:            false     # for debugging purposes, set to true for extra logging
  freeze_whisper:   false      # Freeze the Whisper encoder during training
  use_amp:          false     # Use Automatic Mixed Precision (AMP) for training

  # -- Whisper Encoder ---------------------------------------------------------------------
  dim_whisper:     1280       # Whisper hidden size

  # -- Pooling -----------------------------------------------------------------------------
  pooling:         attention             # attention, mean

  # Parameters for attention pooling
  num_heads:       1                     
  mlp_ratio:       2.0                   
  qkv_bias:        false                 
  qk_scale:        null                 
  drop:            0.0                  
  attn_drop:       0.0                   
  init_scale:      1e-4                 

  # --  projection head (for audio) --------------------------------------------------------
  dim_embed:       768                                 # final embedding size for audio features (= text embedding size)
  dim_hiddens:     [3072, 2048, 2048, 1536]      # hidden sizes for projection head (audio)  

# ───────────────────────── TRAINING ────────────────────────
optimizer: 
  max_lr:              5e-4
  lr:                  1.0e-4    
  weight_decay:        0.01 
  opt_betas:           [0.9, 0.98]
  warmup_steps:        10000
  loss:                mse                # contrastive, cosine, mse 

train:
  epochs:              3
  accumulation_steps:  1         # number of accumulation steps, set to 1 for no gradient accumulation           
  grad_clip:           false
  grad_clip_max_norm:  1.0

  
# ───────────────────────── LOGGING ─────────────
wandb:
  project:   LIE-Whisper-training
  log_dir:   ${hydra:runtime.output_dir}  
  log_steps: 200
  profile:   false        # Enable profiling for performance analysis
  log_logit_scale: false  # Log the logit scale on wandb during training

# ───────────────────────── METRICS ─────────────
metrics:
  topk:      [1,5,10]     # Top-k accuracy metrics to compute during validation